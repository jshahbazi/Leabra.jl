{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "using ProfileVega\n",
    "using Distributions\n",
    "using DataStructures\n",
    "using LinearAlgebra\n",
    "using Parameters\n",
    "using Plots\n",
    "using Random\n",
    "using Statistics\n",
    "\n",
    "import Base.Threads.@spawn\n",
    "\n",
    "function pretty_print(variable, prefix=\"\")\n",
    "    print(prefix)\n",
    "    show(IOContext(stdout, :limit => false), \"text/plain\", variable)\n",
    "    println()\n",
    "end\n",
    "\n",
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reset (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@with_kw mutable struct Unit \n",
    "    act::Float64        = 0.2\n",
    "    avg_ss::Float64     = act\n",
    "    avg_s::Float64      = act\n",
    "    avg_m::Float64      = act\n",
    "    \n",
    "    net::Float64        = 0.0\n",
    "    v_m::Float64        = 0.3\n",
    "    vm_eq::Float64      = 0.3\n",
    "    adapt::Float64      = 0.0\n",
    "    spike::Bool         = false\n",
    "\n",
    "    net_dt::Float64     = 1/1.4\n",
    "    integ_dt::Float64   = 1.0;   # time step constant for integration of cycle dynamics\n",
    "    vm_dt::Float64      = 1/3.3;  # time step constant for membrane potential\n",
    "    l_dn_dt::Float64    = 1/2.5; # time step constant for avg_l decrease\n",
    "    adapt_dt::Float64   = 1/144; # time step constant for adaptation\n",
    "    ss_dt::Float64      = 0.5;    # time step for super-short average\n",
    "    s_dt::Float64       = 0.5;     # time step for short average\n",
    "    m_dt::Float64       = 0.1;     # time step for medium-term average\n",
    "    avg_l_dt::Float64   = 0.1; # time step for long-term average\n",
    "    avg_l_max::Float64  = 1.5; # max value of avg_l\n",
    "    avg_l_min::Float64  = 0.1; # min value of avg_l\n",
    "    avg_l::Float64      = avg_l_min\n",
    "    e_rev_e::Float64    = 1.0;    # excitatory reversal potential\n",
    "    e_rev_i::Float64    = 0.25;  # inhibitory reversal potential\n",
    "    e_rev_l::Float64    = 0.3;  # leak reversal potential\n",
    "    gc_l::Float64       = 0.1;     # leak conductance\n",
    "    thr::Float64        = 0.5;      # normalized \"rate threshold\"\n",
    "    spk_thr::Float64    = 1.2;  # normalized spike threshold\n",
    "    vm_r::Float64       = 0.3;     # reset membrane potential after spike\n",
    "    vm_gain::Float64    = 0.04; # gain that voltage produces on adaptation\n",
    "    spike_gain::Float64 = 0.00805; # effect of spikes on adaptation\n",
    "    l_up_inc::Float64   = 0.2; # increase in avg_l if avg_m has been 'large'     \n",
    "end\n",
    "    \n",
    "function rel_avg_l(u::Unit)::Float64\n",
    "    return (u.avg_l - u.avg_l_min)/(u.avg_l_max - u.avg_l_min)\n",
    "end\n",
    "\n",
    "function l_avg_rel(u::Unit)::Float64\n",
    "    return (u.avg_l - u.avg_l_min)/(u.avg_l_max - u.avg_l_min)\n",
    "end\n",
    "# myunit = Unit()\n",
    "# println(myunit)\n",
    "function cycle(u::Unit, net_raw::Float64, gc_i::Float64)\n",
    "    # Does one Leabra cycle. Called by the layer cycle method.\n",
    "    # net_raw = instantaneous, scaled, received input\n",
    "    # gc_i = fffb inhibition\n",
    "    \n",
    "    ## updating net input\n",
    "    u.net = u.net + u.integ_dt * u.net_dt * (net_raw - u.net);\n",
    "    \n",
    "    ## Finding membrane potential\n",
    "    I_net = u.net*(u.e_rev_e - u.v_m) + u.gc_l*(u.e_rev_l - u.v_m) + gc_i*(u.e_rev_i - u.v_m);\n",
    "    # almost half-step method for updating v_m (adapt doesn't half step)\n",
    "    v_m_h = u.v_m + 0.5*u.integ_dt*u.vm_dt*(I_net - u.adapt);\n",
    "    I_net_h = u.net*(u.e_rev_e - v_m_h) + u.gc_l*(u.e_rev_l - v_m_h) + gc_i*(u.e_rev_i - v_m_h);\n",
    "    u.v_m = u.v_m + u.integ_dt*u.vm_dt*(I_net_h - u.adapt);\n",
    "    u.vm_eq = u.vm_eq + u.integ_dt*u.vm_dt*(I_net_h - u.adapt);\n",
    "    \n",
    "    ## Finding activation\n",
    "    # finding threshold excitatory conductance\n",
    "    g_e_thr = (gc_i*(u.e_rev_i-u.thr) + u.gc_l*(u.e_rev_l-u.thr) - u.adapt) / (u.thr - u.e_rev_e);\n",
    "    # finding whether there's an action potential\n",
    "    if u.v_m > u.spk_thr\n",
    "        u.spike = true;\n",
    "        u.v_m = u.vm_r;\n",
    "    else\n",
    "        u.spike = false;\n",
    "    end\n",
    "    # finding instantaneous rate due to input\n",
    "\n",
    "    if u.vm_eq <= u.thr\n",
    "        new_act = nxx1(u.vm_eq - u.thr)[1];\n",
    "    else\n",
    "        new_act = nxx1(u.net - g_e_thr)[1];\n",
    "    end\n",
    "\n",
    "    # println(\"begin unit stuff\")\n",
    "    # println((u.act))\n",
    "    # println((u.integ_dt))\n",
    "    # println((u.vm_dt))\n",
    "    # println((new_act))\n",
    "    # println(\"end unit stuff\")\n",
    "    # ()\n",
    "    # ()\n",
    "    # ()\n",
    "    # (1,)\n",
    "\n",
    "    # 0.0\n",
    "    # 1.0\n",
    "    # 0.30303030303030304\n",
    "    # [1.4138202523385987e-41] \n",
    "\n",
    "    # update activity\n",
    "    u.act = u.act + u.integ_dt * u.vm_dt * (new_act - u.act);\n",
    "\n",
    "    ## Updating adaptation\n",
    "    u.adapt = u.adapt + u.integ_dt*(u.adapt_dt*(u.vm_gain*(u.v_m - u.e_rev_l)  - u.adapt) + u.spike*u.spike_gain);\n",
    "          \n",
    "    ## updating averages\n",
    "    u.avg_ss = u.avg_ss + u.integ_dt * u.ss_dt * (u.act - u.avg_ss);\n",
    "    u.avg_s = u.avg_s + u.integ_dt * u.s_dt * (u.avg_ss - u.avg_s);\n",
    "    u.avg_m = u.avg_m + u.integ_dt * u.m_dt * (u.avg_s - u.avg_m); \n",
    "end\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "function updt_avg_l(u::Unit)\n",
    "    # This fuction updates the long-term average 'avg_l' \n",
    "    # u = this unit\n",
    "    # Based on the description in:\n",
    "    # https://grey.colorado.edu/ccnlab/index.php/Leabra_Hog_Prob_Fix#Adaptive_Contrast_Impl\n",
    "    \n",
    "    if u.avg_m > 0.2\n",
    "        u.avg_l = u.avg_l + u.avg_l_dt*(u.avg_l_max - u.avg_m);\n",
    "    else\n",
    "        u.avg_l = u.avg_l + u.avg_l_dt*(u.avg_l_min - u.avg_m);\n",
    "    end\n",
    "end\n",
    "\n",
    "function reset(u::Unit)\n",
    "    # This function sets the activity to a random value, and sets\n",
    "    # all activity time averages equal to that value.\n",
    "    # Used to begin trials from a random stationary point.\n",
    "    #u.act = 0.05 + 0.9*rand;\n",
    "    u.act = 0.0;  # Randy begins trials with act=0\n",
    "    u.avg_ss = u.act;\n",
    "    u.avg_s = u.act;\n",
    "    u.avg_m = u.act;\n",
    "    u.avg_l = u.act;\n",
    "    u.net = 0.0;\n",
    "    u.v_m = 0.3;\n",
    "    u.vm_eq = 0.3;\n",
    "    u.adapt = 0.0;            \n",
    "    u.spike = 0.0;            \n",
    "end\n",
    "\n",
    "# function nxx1(points) # replaced by go nxx1 version instead?\n",
    "#     # f = nxx1(points) calculates the noisy x/(x+1) function for\n",
    "#     # all values in the vector 'points'. The returned values come\n",
    "#     # from the convolution of x/(x+1) with a Gaussian function.\n",
    "#     # To avoid calculating the convolution every time, the first\n",
    "#     # time the function is called a vector 'nxoxp1' is created,\n",
    "#     # corresponding to the values of nxx1 at all the points in the\n",
    "#     # vector 'nxx1_dom'. Once these vectors are in the workspace,\n",
    "#     # subsequent calls to nxx1 use interpolation with these\n",
    "#     # vectors in order to calculate their return values.\n",
    "    \n",
    "#     persistent nxoxp1 nxx1_dom\n",
    "    \n",
    "#     if isempty(nxoxp1) || isempty(nxx1_dom)\n",
    "#     # we don't have precalculated vectors for interpolation\n",
    "#         n_points = 2000; # size of the precalculated vectors\n",
    "#         gain = 100; # gain of the xx1 function\n",
    "#         mid = 2; # mid length of the domain\n",
    "#         domain = linspace(-mid,mid,n_points); # will be 'nxx1_dom'\n",
    "#         dom_g = linspace(-2*mid,2*mid,2*n_points); # domain of Gaussian\n",
    "#         values = zeros(1,n_points); # will be 'nxoxp1'\n",
    "#         sd = .005; # standard deviation of the Gaussian\n",
    "#         gaussian = exp(-(dom_g.^2)/(2*sd^2))/(sd*sqrt(2*pi));\n",
    "#         xx1 = max(gain*domain,zeros(1,n_points))./(max(gain*domain,zeros(1,n_points))+1);\n",
    "        \n",
    "#         for p = 1:n_points\n",
    "#             low = n_points - p + 1;\n",
    "#             high = 2*n_points - p;\n",
    "#             values(p) = sum(xx1.*gaussian(low:high));\n",
    "#             values(p) = values(p)/sum(gaussian(low:high));\n",
    "#         end\n",
    "#         nxx1_dom = domain;\n",
    "#         nxoxp1 = values;  \n",
    "#         #disp('First call to nxx1!');\n",
    "#     end\n",
    "#     f = interp1(nxx1_dom,nxoxp1,points,'nearest','extrap');\n",
    "# end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function XX1(x::Float64)\n",
    "    return x / (x + 1)\n",
    "end\n",
    "\n",
    "function XX1GainCor(x::Float64, GainCorRange, NVar, Gain, GainCor)\n",
    "    gainCorFact = (GainCorRange - (x / NVar)) / GainCorRange\n",
    "    if gainCorFact < 0\n",
    "        return XX1(Gain * x)\n",
    "    end\n",
    "    newGain = Gain * (1 - GainCor*gainCorFact)\n",
    "    return XX1(newGain * x)\n",
    "end     \n",
    "\n",
    "@with_kw mutable struct NXX1Params\n",
    "    Thr::Float64            = 0.5\n",
    "    Gain::Int64             = 100\n",
    "    NVar::Float64           = 0.005\n",
    "    VmActThr::Float64       = 0.01\n",
    "    SigMult::Float64        = 0.33\n",
    "    SigMultPow::Float64     = 0.8\n",
    "    SigGain::Float64        = 3.0\n",
    "    InterpRange::Float64    = 0.01\n",
    "    GainCorRange::Float64   = 10.0\n",
    "    GainCor::Float64        = 0.1\n",
    "\n",
    "\tSigGainNVar::Float64    = SigGain / NVar\n",
    "\tSigMultEff::Float64     = SigMult * ((Gain* NVar) ^ SigMultPow)\n",
    "\tSigValAt0::Float64      = 0.5 * SigMultEff\n",
    "\tInterpVal::Float64      = XX1GainCor(InterpRange, GainCorRange, NVar, Gain, GainCor) - SigValAt0\n",
    "end\n",
    "\n",
    "\n",
    "function XX1GainCor(x::Float64, xp::NXX1Params)\n",
    "    gainCorFact = (xp.GainCorRange - (x / xp.NVar)) / xp.GainCorRange\n",
    "    if gainCorFact < 0\n",
    "        return XX1(xp.Gain * x)\n",
    "    end\n",
    "    newGain = xp.Gain * (1 - xp.GainCor*gainCorFact)\n",
    "    return XX1(newGain * x)\n",
    "end  \n",
    "\n",
    "function NoisyXX1(x::Float64, xp::NXX1Params)\n",
    "\tif x < 0\n",
    "\t\treturn xp.SigMultEff / (1 + exp(-(x * xp.SigGainNVar)))\n",
    "\telseif x < xp.InterpRange\n",
    "\t\tinterp = 1 - ((xp.InterpRange - x) / xp.InterpRange)\n",
    "\t\treturn xp.SigValAt0 + interp*xp.InterpVal\n",
    "\telse\n",
    "\t\treturn XX1GainCor(x, xp)\n",
    "    end\n",
    "end\n",
    "\n",
    "function XX1GainCorGain(x::Float64, gain::Float64, xp::NXX1Params)\n",
    "\tgainCorFact = (xp.GainCorRange - (x / xp.NVar)) / xp.GainCorRange\n",
    "\tif gainCorFact < 0\n",
    "\t\treturn XX1(gain * x)\n",
    "    end\n",
    "\tnewGain = gain * (1 - xp.GainCor*gainCorFact)\n",
    "\treturn XX1(newGain * x)\n",
    "end\n",
    "\n",
    "function NoisyXX1Gain(x::Float64, gain::Float64, xp::NXX1Params)\n",
    "\tif x < xp.InterpRange\n",
    "\t\tsigMultEffArg = xp.SigMult * (gain*xp.NVar ^ xp.SigMultPow)\n",
    "\t\tsigValAt0Arg = 0.5 * sigMultEffArg\n",
    "\n",
    "\t\tif x < 0 # sigmoidal for < 0\n",
    "\t\t\treturn sigMultEffArg / (1 + exp(-(x * xp.SigGainNVar)))\n",
    "\t\telse # else x < interp_range\n",
    "\t\t\tinterp = 1 - ((xp.InterpRange - x) / xp.InterpRange)\n",
    "\t\t\treturn sigValAt0Arg + interp*xp.InterpVal\n",
    "        end\n",
    "\telse\n",
    "\t\treturn xp.XX1GainCorGain(x, gain, xp)\n",
    "    end\n",
    "end\n",
    "\n",
    "function test_nxx1()\n",
    "    difTol = 1.0e-7\n",
    "    xx1 = NXX1Params()\n",
    "\n",
    "    tstx = [-0.05, -0.04, -0.03, -0.02, -0.01, 0, .01, .02, .03, .04, .05, .1, .2, .3, .4, .5]\n",
    "    cory = [1.7735989e-14, 7.155215e-12, 2.8866178e-09, 1.1645374e-06, 0.00046864923, 0.094767615, 0.47916666, 0.65277773, 0.742268, 0.7967479, 0.8333333, 0.90909094, 0.95238096, 0.96774197, 0.9756098, 0.98039216]\n",
    "    ny = Array{Float64}(undef, length(tstx))\n",
    "\n",
    "    for i in 1:length(tstx)\n",
    "        ny[i] = NoisyXX1(tstx[i], xx1)\n",
    "        dif = abs(ny[i] - cory[i])\n",
    "        if dif > difTol # allow for small numerical diffs\n",
    "            println(\"XX1 err: dix: $i, x: $(tstx[i]), y: $(ny[i]), cor y: $(cory[i]), dif: $dif\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function nxx1(points)\n",
    "    xp = NXX1Params()\n",
    "    results = Array{Float64}(undef, length(points))\n",
    "    for (index,value) in enumerate(points)\n",
    "        results[index] = NoisyXX1(value, xp)\n",
    "    end\n",
    "    return results\n",
    "end\n",
    "\n",
    "test_nxx1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reset (generic function with 3 methods)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net.layers[i].wt = Array{Unit, 2}(undef, (net.layers[i].N, lay_inp_size[i]))\n",
    "\n",
    "@with_kw mutable struct Layer \n",
    "    units::Array{Unit}\n",
    "    pct_act_scale::Float64\n",
    "    acts_p_avg::Float64\n",
    "    netin_avg::Float64\n",
    "    wt::Array{Float64, 2}\n",
    "    ce_wt::Array{Float64, 2}\n",
    "    N::Int64\n",
    "    fbi::Float64\n",
    "\n",
    "    ff::Float64     = 1.0\n",
    "    ff0::Float64    = 0.1\n",
    "    fb::Float64     = 0.5\n",
    "    fb_dt::Float64  = 1/1.4 # time step for fb inhibition (fb_tau=1.4)\n",
    "    gi::Float64     = 2.0\n",
    "    avg_act_dt::Float64 = 0.01\n",
    "end\n",
    "\n",
    "function layer(dims::Tuple{Int64, Int64} = (1,1))\n",
    "    N = dims[1]*dims[2]\n",
    "    # lay.units = unit.empty;  # so I can make the assignment below\n",
    "    # lay.units(lay.N,1) = unit; # creating all units\n",
    "    # # notice how the unit array is 1-D. The 2-D structure of the\n",
    "    # # layer doesn't have meaning in this part of the code \n",
    "    \n",
    "    # units = Array{Unit, 1}(undef, N)\n",
    "    units = [Unit() for i in 1:N]\n",
    "    \n",
    "    ff     = 1.0\n",
    "    ff0    = 0.1\n",
    "    fb     = 0.5\n",
    "    fb_dt  = 1/1.4 # time step for fb inhibition (fb_tau=1.4)\n",
    "    gi     = 2.0\n",
    "    avg_act_dt = 0.01\n",
    "    acts_avg = 0.2 # should be the average but who knows\n",
    "    avg_act_n =  1.0 # should be the avg\n",
    "    pct_act_scale = 1/(avg_act_n + 2);\n",
    "\n",
    "    lay = Layer(\n",
    "        units = units,\n",
    "        pct_act_scale = pct_act_scale,\n",
    "        acts_p_avg = acts_avg,\n",
    "        netin_avg = 0,\n",
    "        # wt = Array{Float64, 2}(undef, (1,1)),\n",
    "        wt = zeros(Float64, (1,1)),\n",
    "        # ce_wt = Array{Float64, 2}(undef, (1,1)),\n",
    "        ce_wt = zeros(Float64, (1,1)),\n",
    "        N = N,\n",
    "        fbi = fb * acts_avg\n",
    "    )\n",
    "\n",
    "    return lay\n",
    "end\n",
    "\n",
    "function acts_avg(lay::Layer)\n",
    "    # get the value of acts_avg, the mean of unit activities\n",
    "    return mean(activities(lay));\n",
    "end\n",
    "\n",
    "function activities(lay::Layer)\n",
    "    # ## returns a vector with the activities of all units\n",
    "    # acts = Array{Float64, 1}(undef, lay.N)\n",
    "    acts = zeros(Float64, lay.N)\n",
    "    for (index,unit) in enumerate(lay.units)\n",
    "        acts[index] = unit.act\n",
    "    end\n",
    "    return transpose(acts)\n",
    "end\n",
    "\n",
    "# function scaled_acts(lay::Layer)\n",
    "#     # ## returns a vector with the scaled activities of all units\n",
    "#     acts = Array{Float64, 1}(undef, lay.N)\n",
    "#     for (index,unit) in enumerate(lay.units)\n",
    "#         acts[index] = unit.act\n",
    "#     end\n",
    "#     return lay.pct_act_scale .* collect(transpose(acts))\n",
    "# end\n",
    "\n",
    "function cycle(lay::Layer, raw_inputs::Array{Float64}, ext_inputs::Array{Float64})\n",
    "    ## this function performs one Leabra cycle for the layer\n",
    "    #raw_inputs = An Ix1 matrix, where I is the total number of inputs\n",
    "    #             from all layers. Each input has already been scaled\n",
    "    #             by the pct_act_scale of its layer of origin and by\n",
    "    #             the wt_scale_rel factor.\n",
    "    #ext_inputs = An Nx1 matrix denoting inputs that don't come\n",
    "    #             from another layer, where N is the number of\n",
    "    #             units in this layer. An empty matrix indicates\n",
    "    #             that there are no external inputs.\n",
    "     \n",
    "    ## obtaining the net inputs            \n",
    "    #netins = lay.wt*raw_inputs;\n",
    "    # println(size(lay.ce_wt))\n",
    "    # println(size(raw_inputs))\n",
    "    netins = lay.ce_wt * raw_inputs;  # you use contrast-enhanced weights\n",
    "    # if !(isempty(ext_inputs))\n",
    "    if any(ext_inputs) .> 0.0\n",
    "        netins = netins .+ ext_inputs;\n",
    "    end\n",
    "    # (100, 50)\n",
    "    # (50,)\n",
    "    # DimensionMismatch: arrays could not be broadcast to a common size; \n",
    "    # got a dimension with lengths 100 and 50\n",
    "\n",
    "\n",
    "    ## obtaining inhibition\n",
    "    lay.netin_avg = mean(netins); \n",
    "    ffi = lay.ff * max(lay.netin_avg - lay.ff0, 0);\n",
    "    # println(\"cycle before: $(lay.fbi)\")\n",
    "    lay.fbi = lay.fbi + lay.fb_dt * (lay.fb * acts_avg(lay) - lay.fbi);\n",
    "    # println(\"cycle after: $(lay.fbi)\")\n",
    "    gc_i = lay.gi * (ffi + lay.fbi); \n",
    "    \n",
    "    ## calling the cycle method for all units\n",
    "    # function cycle(u::Unit, net_raw::Float64, gc_i::Float64)\n",
    "    for i in 1:lay.N  # a parfor here?\n",
    "        #lay.units[i].cycle(i, netins[i], gc_i);\n",
    "        cycle(lay.units[i], netins[i], gc_i);\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "function averages(lay::Layer)\n",
    "    # Returns the s,m,l averages in the layer as vectors.\n",
    "    # Notice that the ss average is not returned, and avg_l is not\n",
    "    # updated before being returned.\n",
    "    \n",
    "    # [x^2 for x in 0:9 if x > 5] \n",
    "    # 4-element Array{Int64,1}:\n",
    "    #  36\n",
    "    #  49\n",
    "    #  64\n",
    "    #  81\n",
    "\n",
    "    avg_s = [unit.avg_s for unit in lay.units]\n",
    "    avg_m = [unit.avg_m for unit in lay.units]\n",
    "    avg_l = [unit.avg_l for unit in lay.units]\n",
    "    # print(size(avg_s), \" \", size(avg_m), \" \",size(avg_l))\n",
    "    return (avg_s, avg_m, avg_l)\n",
    "end\n",
    "\n",
    "function rel_avg_l(lay::Layer)\n",
    "    # Returns the relative values of avg_l. These are the dependent\n",
    "    # variables rel_avg_l in all units used in latest XCAL\n",
    "    # l_avg_rel = [this.units.rel_avg_l];\n",
    "    return [rel_avg_l(unit) for unit in lay.units]\n",
    "\n",
    "end\n",
    "\n",
    "function updt_avg_l(lay::Layer)\n",
    "    # updates the long-term average (avg_l) of all the units in the\n",
    "    # layer. Usually done after a plus phase.\n",
    "    for i in 1:lay.N \n",
    "        updt_avg_l(lay.units[i])\n",
    "    end            \n",
    "end\n",
    "\n",
    "function updt_long_avgs(lay::Layer)\n",
    "    # updates the acts_p_avg and pct_act_scale variables.\n",
    "    # These variables update at the end of plus phases instead of\n",
    "    # cycle by cycle. \n",
    "    # This version assumes full connectivity when updating\n",
    "    # pct_act_scale. If partial connectivity were to be used, this\n",
    "    # should have the calculation in WtScaleSpec::SLayActScale, in\n",
    "    # LeabraConSpec.cpp \n",
    "    lay.acts_p_avg = lay.acts_p_avg + lay.avg_act_dt * (acts_avg(lay) - lay.acts_p_avg);\n",
    "                 \n",
    "    r_avg_act_n = max(round(lay.acts_p_avg * lay.N), 1);\n",
    "    lay.pct_act_scale = 1/(r_avg_act_n + 2);  \n",
    "    # println(\"lay.acts_p_avg: $(lay.acts_p_avg)\")\n",
    "    # println(\"r_avg_act_n: $(r_avg_act_n)\")\n",
    "    # println(\"lay.pct_act_scale: $(lay.pct_act_scale)\")\n",
    "end\n",
    "\n",
    "function reset(lay::Layer)\n",
    "    # This function sets the activity of all units to random values, \n",
    "    # and all other dynamic variables are also set accordingly.            \n",
    "    # Used to begin trials from a random stationary point.\n",
    "    # The activity values may also be set to zero (see unit.reset)\n",
    "    for i in 1:lay.N \n",
    "        reset(lay.units[i])\n",
    "    end         \n",
    "end\n",
    "\n",
    "# mylayer = layer((1,1))\n",
    "# activities(mylayer)\n",
    "# acts_avg(mylayer)\n",
    "# scaled_acts(mylayer)\n",
    "# averages(mylayer)\n",
    "# rel_avg_l(mylayer)\n",
    "# updt_avg_l(mylayer)\n",
    "# updt_long_avgs(mylayer)\n",
    "# reset(mylayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "network (generic function with 1 method)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@with_kw mutable struct Network\n",
    "    layers::Array{Layer}\n",
    "    connections::Array{Float64, 2}\n",
    "    n_lays::Int64\n",
    "    n_units::Int64\n",
    "    lrate::Float64\n",
    "\n",
    "    avg_l_lrn_max::Float64 = 0.01; # max amount of \"BCM\" learning in XCAL\n",
    "    avg_l_lrn_min::Float64 = 0.0;  # min amount of \"BCM\" learning in XCAL\n",
    "    m_in_s::Float64 = 0.1; # proportion of medium to short term avgs. in XCAL\n",
    "    m_lrn::Float64 = 1;  # proportion of error-driven learning in XCAL\n",
    "    d_thr::Float64 = 0.0001; # threshold for XCAL \"check mark\" function\n",
    "    d_rev::Float64 = 0.1;    # reversal value for XCAL \"check mark\" function\n",
    "    off::Float64 = 1.0;    # 'offset' in the SIG function for contrast enhancement\n",
    "    gain::Float64 = 6.0;   # gain in the SIG function for contrast enhancement\n",
    "end\n",
    "\n",
    "function m1(net::Network)\n",
    "    # obtains the m1 factor: the slope of the left-hand line in the\n",
    "    # \"check mark\" XCAL function. Notice it includes the negative\n",
    "    # sign.\n",
    "    m = (net.d_rev - 1) / net.d_rev;\n",
    "end\n",
    "\n",
    "function network(dim_layers, connections, w0)\n",
    "    # constructor to the network class.\n",
    "    # dim_layers = 1-D cell array. dim_layers{i} is a vector [j,k], \n",
    "    #              where j is the number of rows in layer i, and k is\n",
    "    #              the number of columns in layer i.\n",
    "    # connections = a 2D array. If layer j sends projections to \n",
    "    #               layer i, then connections[i,j] = c > 0; 0 otherwise\n",
    "    # w0 = w0 is a cell array. w0{i,j} is the weight matrix with the\n",
    "    #      initial weights for the connections from layer j to i             \n",
    "    \n",
    "    ## Initial test of argument dimensions\n",
    "    n_lay = length(dim_layers);  # number of layers\n",
    "    (nrc, ncc) = size(connections);\n",
    "    # if nrc != ncc\n",
    "    #     prinitln('Non-square connections matrix in network constructor');\n",
    "    # end\n",
    "    # if nrc != n_lay\n",
    "    #     prinitln('Number of layers inconsistent with connection matrix in network constructor');\n",
    "    # end\n",
    "    # if sum(size(w0) == size(connections)) < 2\n",
    "    #     prinitln('Inconsistent dimensions between initial weights and connectivity specification in network constructor');\n",
    "    # end   \n",
    "    # if min(min(connections)) < 0\n",
    "    #     prinitln('Negative projection strengths between layers are not allowed in connections matrix');\n",
    "    # end\n",
    "   \n",
    "    net = Network(\n",
    "        layers = Array{Layer, 1}(undef, n_lay),\n",
    "        # layers = \n",
    "        # connections = Array{Float64, 2}(undef, (n_lay, n_lay)),\n",
    "        connections = zeros(Float64, (n_lay, n_lay)),\n",
    "        n_lays = n_lay,\n",
    "        n_units = 0,   # counting number of units in the network\n",
    "        lrate = 0.1, # set default learning rate\n",
    "    )\n",
    "\n",
    "    ## Normalizing the rows of 'connections' so they add to 1\n",
    "    # println(typeof(connections))\n",
    "    # println(connections)\n",
    "    row_sums = sum(connections, dims=2)\n",
    "    for row in 1:n_lay\n",
    "        if row_sums[row] > 0\n",
    "            connections[row,:] = connections[row,:] ./ row_sums[row]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    for i in 1:n_lay\n",
    "        # new_layer = Layer()\n",
    "        net.layers[i] = layer(dim_layers[i]);\n",
    "        net.n_units = net.n_units + net.layers[i].N;\n",
    "    end              \n",
    "\n",
    "\n",
    "    ## Second test of argument dimensions\n",
    "    for i in 1:n_lay\n",
    "        for j in 1:n_lay\n",
    "            if w0[i,j] == 0.0\n",
    "                # println(\"empty\")\n",
    "                if connections[i,j] > 0.0\n",
    "                    println(\"Connected layers have no connection weights\");\n",
    "                end\n",
    "            else\n",
    "                # println(\"nonempty\")\n",
    "                if connections[i,j] == 0.0\n",
    "                    println(\"Non-empty weight matrix for unconnected layers\");\n",
    "                end\n",
    "                \n",
    "                # println(i,\" \",j)\n",
    "                # # println(size(w0[2]))\n",
    "                # println(w0[i,j])\n",
    "                # println(connections[i,j])\n",
    "                # # println(net.layers[i].N, \" \", net.layers[j].N)\n",
    "                (r,c) = size(w0[i,j]);\n",
    "                if net.layers[i].N != r || net.layers[j].N != c\n",
    "                    println(\"Initial weigths are inconsistent with layer dimensions\");\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    net.connections = connections; # assigning layer connection matrix\n",
    "    \n",
    "    ## Setting the inital weights for each layer\n",
    "    # first find how many units project to the layer in all the network\n",
    "    lay_inp_size::Array{Int64} = zeros(1,n_lay);\n",
    "    for i in 1:n_lay\n",
    "        for j in 1:n_lay\n",
    "            if connections[i,j] > 0  # if layer j projects to layer i\n",
    "                lay_inp_size[i] = lay_inp_size[i] + net.layers[j].N;\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # println(lay_inp_size)\n",
    "    # println()\n",
    "    # add the weights for each entry in w0 as a group of columns in wt\n",
    "    for i in 1:n_lay\n",
    "        # net.layers[i].wt = Array{Float64, 2}(undef, (net.layers[i].N, lay_inp_size[i]))\n",
    "        net.layers[i].wt = zeros(Float64, (net.layers[i].N, lay_inp_size[i]))\n",
    "        # println(net.layers[i].wt)\n",
    "        index = 1;\n",
    "        for j = 1:n_lay\n",
    "            if connections[i,j] > 0  # if layer j projects to layer i\n",
    "                nj = net.layers[j].N;\n",
    "                # println(i, \" \", j, \" \", nj, \" \", index, \" \", size(net.layers[i].wt), \" \", size(w0[i,j]))\n",
    "                net.layers[i].wt[:,index:index+nj-1] = w0[i,j];\n",
    "                index = index + nj;\n",
    "            end\n",
    "        end\n",
    "    end \n",
    "    \n",
    "    # set the contrast-enhanced version of the weights\n",
    "    for lay in 1:n_lay\n",
    "        net.layers[lay].ce_wt = 1 ./ (1 .+ (net.off * (1 .- net.layers[lay].wt) ./ net.layers[lay].wt) .^ net.gain)\n",
    "    end\n",
    "\n",
    "    return net\n",
    "end\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XCAL_learn (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function XCAL_learn(net::Network)\n",
    "    # XCAL_learn() applies the XCAL learning equations in order to\n",
    "    # modify the weights in the network. This is typically done at\n",
    "    # the end of a plus phase. The equations used come from:\n",
    "    # https://grey.colorado.edu/ccnlab/index.php/Leabra_Hog_Prob_Fix#Adaptive_Contrast_Impl\n",
    "    # Soft weight bounding and contrast enhancememnt are as in:\n",
    "    # https://grey.colorado.edu/emergent/index.php/Leabra\n",
    "    \n",
    "    ## updating the long-term averages\n",
    "    for lay in 1:net.n_lays\n",
    "        updt_avg_l(net.layers[lay]); \n",
    "    end\n",
    "    \n",
    "    ## Extracting the averages for all layers            \n",
    "    avg_s = Array{Vector{Float64}, 1}(undef, net.n_lays)\n",
    "    avg_m = Array{Vector{Float64}, 1}(undef, net.n_lays)\n",
    "    avg_l = Array{Vector{Float64}, 1}(undef, net.n_lays)\n",
    "    avg_s_eff = Array{Vector{Float64}, 1}(undef, net.n_lays)\n",
    "\n",
    "    for lay in 1:net.n_lays\n",
    "        # println(averages(net.layers[lay]))\n",
    "        (avg_s[lay], avg_m[lay], avg_l[lay]) = averages(net.layers[lay]) # layer.averages() function\n",
    "        # println(size(net.m_in_s), \" \", size(avg_m[lay]), \" \", size(avg_s[lay]))\n",
    "        avg_s_eff[lay] = net.m_in_s * avg_m[lay] + (1 - net.m_in_s) * avg_s[lay];\n",
    "        # println(avg_s_eff[lay])\n",
    "        # at net point, the avg_X vectors are row vectors\n",
    "    end\n",
    "    \n",
    "    ## obtaining avg_l_lrn\n",
    "    avg_l_lrn = Array{Vector{Float64}, 1}(undef, net.n_lays)\n",
    "    # println(size(avg_l_lrn))\n",
    "    for lay in 1:net.n_lays\n",
    "        # println(net.avg_l_lrn_min .+ rel_avg_l(net.layers[lay]) .* (net.avg_l_lrn_max - net.avg_l_lrn_min))\n",
    "        # println(typeof(avg_l_lrn[lay]))\n",
    "        avg_l_lrn[lay] = net.avg_l_lrn_min .+ rel_avg_l(net.layers[lay]) .* (net.avg_l_lrn_max - net.avg_l_lrn_min)\n",
    "    end\n",
    "    #assignin('base','avg_l_lrn',avg_l_lrn) # if you wanna see                  \n",
    "    \n",
    "    ## For each connection matrix, calculate the intermediate vars.\n",
    "    srs = Array{Array{Float64, 2}, 2}(undef, (net.n_lays,net.n_lays)) # srs{i,j} = matrix of short-term averages\n",
    "                                # where the rows correspond to the\n",
    "                                # units of the receiving layer, columns\n",
    "                                # to units of the sending layer.\n",
    "    srm = Array{Array{Float64, 2}, 2}(undef, (net.n_lays,net.n_lays)) # ditto for avg_m\n",
    "    # println(\"size(srs): \",size(srs))\n",
    "    for rcv in 1:net.n_lays\n",
    "        for snd in rcv:net.n_lays\n",
    "            # notice we only calculate the 'upper triangle' of the\n",
    "            # cell arrays because of symmetry\n",
    "            if net.connections[rcv,snd] > 0 || net.connections[snd,rcv] > 0\n",
    "                \n",
    "                # println(\"size(transpose(avg_s_eff)): $(size(transpose(avg_s_eff[rcv])))   size(avg_s_eff[snd]): $(size(avg_s_eff[snd]))\")\n",
    "                # println(avg_s_eff[rcv])\n",
    "                # println(avg_s_eff[snd])\n",
    "                # println(\"size(avg_s_eff[rcv]): $(size(avg_s_eff[rcv])) --- size(avg_s_eff[snd]): $(size(avg_s_eff[snd]))\")\n",
    "                srs[rcv,snd] = avg_s_eff[rcv] * transpose(avg_s_eff[snd])\n",
    "                # println(size(srs[rcv,snd]))\n",
    "                srm[rcv,snd] = avg_m[rcv] * transpose(avg_m[snd])\n",
    "\n",
    "                if snd != rcv # using symmetry\n",
    "                    srs[snd,rcv] = transpose(srs[rcv,snd])\n",
    "                    srm[snd,rcv] = transpose(srm[rcv,snd])                             \n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ## calculate the weight changes\n",
    "    dwt = Array{Array{Float64, 2}, 2}(undef, (net.n_lays,net.n_lays)) # dwt{i,j} is the matrix of weight changes\n",
    "                                # for the weights from layer j to i\n",
    "    for rcv in 1:net.n_lays\n",
    "        for snd in 1:net.n_lays\n",
    "            if net.connections[rcv,snd] > 0\n",
    "                sndN = net.layers[snd].N;\n",
    "                outer = (1, sndN)\n",
    "                # println(\"srs: \",size(srs[rcv,snd]))\n",
    "                # println(sndN)\n",
    "                # println(size(((avg_l[rcv]))))\n",
    "                # println(\"repeat: \",size(transpose(repeat(transpose(avg_l[rcv]), sndN))))\n",
    "                # println(size((net.lrate .* ( net.m_lrn .* xcal(net, srs[rcv,snd], srm[rcv,snd])))))\n",
    "                # println(size(srs[rcv,snd]), \" \", size(repeat(transpose(avg_l[rcv]), sndN)))\n",
    "                # println(size(xcal(net, srs[rcv,snd], transpose(repeat(transpose(avg_l[rcv]), sndN)))))\n",
    "                # println(size(transpose(avg_l_lrn[rcv])))\n",
    "                # println(size((expand(avg_l_lrn[rcv], 2, 25))))\n",
    "                # println(size(srs[rcv,snd])[2])\n",
    "                dwt[rcv,snd] = net.lrate .* ( net.m_lrn .* xcal(net, srs[rcv,snd], srm[rcv,snd]) .+ ((expand(avg_l_lrn[rcv], 2, size(srs[rcv,snd])[2])) .* xcal(net, srs[rcv,snd], transpose(repeat(transpose(avg_l[rcv]), sndN)))));\n",
    "                                    #   100 x 25                                                 +    1 x 100                 *        100 x 25\n",
    "                                    #   100 x 25                                                 +    100 x 1                 *        100 x 25\n",
    "                # println(\"---\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    # println(size(dwt))\n",
    "    ## update weights (with weight bounding)\n",
    "    for rcv in 1:net.n_lays                \n",
    "        # DW = Array{Matrix{Float64}, 2}(undef, (net.n_lays,net.n_lays))\n",
    "        DW = zeros(Float64, (net.n_lays,net.n_lays))\n",
    "        hit = false\n",
    "        for snd in 1:net.n_lays\n",
    "            # println((net.connections[rcv,snd]))\n",
    "            if net.connections[rcv,snd] > 0\n",
    "                # println(rcv, \" - \", snd)\n",
    "                if !hit\n",
    "                    DW = dwt[rcv,snd]\n",
    "                else\n",
    "                    DW = hcat(DW, dwt[rcv,snd]);\n",
    "                end\n",
    "                hit = true\n",
    "            end\n",
    "        end\n",
    "        # println((dwt))\n",
    "        # println(size(DW))\n",
    "        # if all(DW[rcv] .== 0.0)\n",
    "        #     println(\"hi\")\n",
    "        # end\n",
    "        if hit #!isempty(DW)\n",
    "            # Here's the weight bounding part, as in the CCN book\n",
    "            idxp = net.layers[rcv].wt .> 0\n",
    "            idxn = .!idxp # maps ! function onto the BitArray\n",
    "            \n",
    "            # println(DW)\n",
    "            net.layers[rcv].wt[idxp] = net.layers[rcv].wt[idxp] .+ (1 .- net.layers[rcv].wt[idxp]) .* DW[idxp];\n",
    "            net.layers[rcv].wt[idxn] = net.layers[rcv].wt[idxn] .+ net.layers[rcv].wt[idxn] .* DW[idxn];\n",
    "            #disp(['mean weight change: ' num2str(sum(sum(abs(DW)))/numel(DW))]);\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ## set the contrast-enhanced version of the weights\n",
    "    for lay in 1:net.n_lays\n",
    "        net.layers[lay].ce_wt = 1 ./ (1 .+ (net.off .* (1 .- net.layers[lay].wt) ./ net.layers[lay].wt) .^ net.gain)\n",
    "        # println(size(net.layers[lay].ce_wt))\n",
    "        # println((net.layers[lay].ce_wt))\n",
    "    end\n",
    "end # end XCAL_learn method \n",
    "\n",
    "# XCAL_learn(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expand (generic function with 1 method)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function expand(x, dim, copies)\n",
    "    sz = size(x)\n",
    "    rep = ntuple(d->d==dim ? copies : 1, length(sz)+1)\n",
    "    new_size = ntuple(d->d<dim ? sz[d] : d == dim ? 1 : sz[d-1], length(sz)+1)\n",
    "    return repeat(reshape(x, new_size), outer=rep)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reset (generic function with 3 methods)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updt_long_avgs(net::Network)\n",
    "    # updates the acts_p_avg and pct_act_scale variables for all layers. \n",
    "    # These variables update at the end of plus phases instead of\n",
    "    # cycle by cycle. The avg_l values are not updated here.\n",
    "    # This version assumes full connectivity when updating\n",
    "    # pct_act_scale. If partial connectivity were to be used, this\n",
    "    # should have the calculation in WtScaleSpec::SLayActScale, in\n",
    "    # LeabraConSpec.cpp \n",
    "    for lay in 1:net.n_lays\n",
    "        updt_long_avgs(net.layers[lay])\n",
    "    end\n",
    "end\n",
    "\n",
    "function m1(net::Network)\n",
    "    # obtains the m1 factor: the slope of the left-hand line in the\n",
    "    # \"check mark\" XCAL function. Notice it includes the negative\n",
    "    # sign.\n",
    "    return (net.d_rev - 1.0) / net.d_rev;\n",
    "end\n",
    "        \n",
    "function xcal(net::Network, x, th)\n",
    "    # println(\"sizes: \",size(x), \" \", size(th))\n",
    "    @assert size(x) == size(th)\n",
    "    # this function implements the \"check mark\" function in XCAL.\n",
    "    # x = an array of abscissa values.\n",
    "    # th = an array of threshold values, same size as x\n",
    "    # println(size(x), \" \", x)\n",
    "    f = zeros(size(x));\n",
    "    temp = x .> net.d_thr\n",
    "    # println(x, \" \", net.d_thr, \": \", temp)\n",
    "    temp2 = x .< (net.d_rev * th)\n",
    "    # println(x, \" \", (net.d_rev * th), \"  \", temp2)\n",
    "    idx1 = temp .& temp2\n",
    "    # println(idx1)\n",
    "    idx2 = x .>= (net.d_rev * th);\n",
    "    # println(idx2)\n",
    "    # println(m1(net))\n",
    "\n",
    "    f[idx1] = m1(net) * x[idx1];\n",
    "    f[idx2] = x[idx2] - th[idx2];\n",
    "    return f      \n",
    "end\n",
    "\n",
    "function reset(net::Network)\n",
    "    # This function sets the activity of all units to random values, \n",
    "    # and all other dynamic variables are also set accordingly.            \n",
    "    # Used to begin trials from a random stationary point.\n",
    "    for lay = 1:net.n_lays\n",
    "        reset(net.layers[lay])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_weights (generic function with 1 method)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function set_weights(net::Network, w::Array{Matrix{Float64}, 2})\n",
    "    # This function receives a cell array w, which is like the cell\n",
    "    # array w0 in the constructor: w{i,j} is the weight matrix with\n",
    "    # the initial weights for the connections from layer j to layer\n",
    "    # i. The weights are set to the values of w.\n",
    "    # This whole function is a slightly modified copypasta of the\n",
    "    # constructor.\n",
    "    \n",
    "    ## First we test the dimensions of w\n",
    "    if sum(size(w) == size(net.connections)) < 2\n",
    "        throw(\"Inconsistent dimensions between weights and connectivity specification in set_weights\");\n",
    "    end\n",
    "\n",
    "    for i in 1:net.n_lays\n",
    "        for j in 1:net.n_lays\n",
    "            # if isempty(w[i,j])\n",
    "            if all(w[i,j] .== 0.0)\n",
    "                if net.connections[i,j] > 0\n",
    "                    throw(\"Connected layers have no connection weights\");\n",
    "                end\n",
    "            else\n",
    "                if net.connections[i,j] == 0\n",
    "                    throw(\"Non-empty weight matrix for unconnected layers\");\n",
    "                end\n",
    "                (r,c) = size(w[i,j]);\n",
    "                if net.layers[i].N != r || net.layers[j].N != c\n",
    "                    throw(\"Initial weights are inconsistent with layer dimensions\");\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ## Now we set the weights\n",
    "    # first find how many units project to the layer in all the network\n",
    "    lay_inp_size = zeros(1,net.n_lays)\n",
    "    for i in 1:net.n_lays\n",
    "        for j in 1:net.n_lays\n",
    "            if net.connections[i,j] > 0  # if layer j projects to layer i\n",
    "                lay_inp_size[i] = lay_inp_size[i] + net.layers[j].N;\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # add the weights for each entry in w0 as a group of columns in wt\n",
    "    for i in 1:net.n_lays\n",
    "        net.layers[i].wt = zeros(net.layers[i].N,lay_inp_size[i]);\n",
    "        index = 1;\n",
    "        for j in 1:net.n_lays\n",
    "            if net.connections[i,j] > 0  # if layer j projects to layer i\n",
    "                nj = net.layers[j].N;\n",
    "                net.layers[i].wt[:,index:index+nj-1] = w[i,j];\n",
    "                index = index + nj;\n",
    "            end\n",
    "        end\n",
    "    end   \n",
    "    \n",
    "    # set the contrast-enhanced version of the weights\n",
    "    for lay in 1:net.n_lays\n",
    "        net.layers[lay].ce_wt = 1 ./ (1 .+ (net.off .* (1 .- net.layers[lay].wt) ./ net.layers[lay].wt) .^ net.gain)\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_weights(netnet::Network)\n",
    "    # This function returns a 2D cell array w.\n",
    "    # w{rcv,snd} contains the weight matrix for the projections from\n",
    "    # layer snd to layer rcv.\n",
    "    w = Array{Array{Float64, 2}, 2}(undef, (net.n_lays,net.n_lays))\n",
    "    for rcv in 1:net.n_lays\n",
    "        idx1 = 1; # column where the weights from layer 'snd' start\n",
    "        for snd in 1:net.n_lays\n",
    "            if net.connections[rcv,snd] > 0\n",
    "                Nsnd = net.layers[rcv].N;\n",
    "                w[rcv,snd] = net.layers[rcv].wt[:,idx1:idx1+Nsnd-1];\n",
    "                idx1 = idx1 + Nsnd;\n",
    "            end\n",
    "        end               \n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cycle (generic function with 3 methods)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function scaled_acts(lay::Layer)\n",
    "    # ## returns a vector with the scaled activities of all units\n",
    "    acts = Array{Float64, 1}(undef, lay.N)\n",
    "    for (index,unit) in enumerate(lay.units)\n",
    "        acts[index] = unit.act\n",
    "    end\n",
    "    return lay.pct_act_scale .* acts #collect(transpose(acts))\n",
    "end\n",
    "\n",
    "function clamped_cycle(u::Unit, input)\n",
    "    # This function performs one cycle of the unit when its activty\n",
    "    # is clamped to an input value. The activity is set to be equal\n",
    "    # to the input, and all the averages are updated accordingly.\n",
    "    \n",
    "    ## Clamping the activty to the input\n",
    "    u.act = input;\n",
    "    \n",
    "    ## updating averages\n",
    "    u.avg_ss = u.avg_ss + u.integ_dt * u.ss_dt * (u.act - u.avg_ss);\n",
    "    u.avg_s = u.avg_s + u.integ_dt * u.s_dt * (u.avg_ss - u.avg_s);\n",
    "    u.avg_m = u.avg_m + u.integ_dt * u.m_dt * (u.avg_s - u.avg_m);\n",
    "end\n",
    "\n",
    "function clamped_cycle(lay::Layer, input::Array{Float64})\n",
    "    # sets all unit activities equal to the input and updates all\n",
    "    # the variables as in the cycle function.\n",
    "    # input = vector specifying the activities of all units\n",
    "    for i = 1:lay.N  # parfor ?\n",
    "        # function clamped_cycle(u::Unit, input)\n",
    "        clamped_cycle(lay.units[i], input[i]);\n",
    "    end\n",
    "    # println(\"clamped_cycle before: $(lay.fbi)\")\n",
    "    # updating inhibition for the next cycle\n",
    "    # println(\"clamped_cycle acts_avg before: $(lay.fb) $(acts_avg(lay))\")\n",
    "    lay.fbi = lay.fb * acts_avg(lay)\n",
    "    # println(\"clamped_cycle after: $(lay.fbi)\")\n",
    "end\n",
    "\n",
    "function cycle(net::Network, inputs::Vector{Array{Float64}}, clamp_inp::Bool)\n",
    "    # this function calls the cycle method for all layers.\n",
    "    # inputs = a cell array. inputs{i} is  a matrix that for layer i\n",
    "    #          specifies the external input to each of its units.\n",
    "    #          An empty matrix denotes no input to that layer.\n",
    "    # clamp_inp = a binary flag. 1 -> layers are clamped to their\n",
    "    #             input value. 0 -> inputs summed to netins.\n",
    "    \n",
    "    ## Testing the arguments and reshaping the input\n",
    "    # if ~iscell(inputs)\n",
    "    #     error('First argument to cycle function should be an inputs cell');\n",
    "    # end\n",
    "    # if length(inputs) ~= net.n_lays\n",
    "    #     error('Number of layers inconsistent with number of inputs in network cycle');\n",
    "    # end\n",
    "    for inp in 1:net.n_lays  # reshaping inputs into column vectors\n",
    "        if any(inputs[inp] .> 0.0) #if ~isempty(inputs[inp])\n",
    "            inputs[inp] = reshape(inputs[inp], net.layers[inp].N, 1);\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ## First we set all clamped layers to their input values\n",
    "    clamped_lays = zeros(1, net.n_lays);\n",
    "    if clamp_inp\n",
    "        for lay in 1:net.n_lays\n",
    "            if any(inputs[lay] .> 0.0) # if ~isempty(inputs[lay])\n",
    "                clamped_cycle(net.layers[lay], inputs[lay]);\n",
    "                clamped_lays[lay] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ## We make a copy of the scaled activity for all layers\n",
    "    # scaled_acts = zeros(Float64, 1, net.n_lays);\n",
    "    scaled_acts_array = Array{Vector{Float64}}(undef, net.n_lays)\n",
    "    for lay in 1:net.n_lays\n",
    "        # println((scaled_acts(net.layers[lay])))\n",
    "        scaled_acts_array[lay] = scaled_acts(net.layers[lay])\n",
    "    end\n",
    "    \n",
    "    ## For each unclamped layer, we put all its scaled inputs in one\n",
    "    #  column vector, and call its cycle function with that vector\n",
    "    for recv in 1:net.n_lays\n",
    "        if all(clamped_lays[recv] .== 0.0) # if the layer is not clamped\n",
    "            # for each 'recv' layer we find its input vector\n",
    "            long_input = zeros(1, net.n_units); # preallocating for speed\n",
    "            n_inps = 0;  # will have the # of input units to 'recv'\n",
    "            n_sends = 0; # will have the # of layers sending to 'recv'\n",
    "            conns = (net.connections[recv, :] .> 0.0)\n",
    "            # println(net.connections[recv, :])\n",
    "            # println(conns)\n",
    "            wt_scale_rel = net.connections[recv, conns];\n",
    "            # wt_scale_rel is the non-zero entires of row 'recv'\n",
    "            for send in 1:net.n_lays\n",
    "                if net.connections[recv, send] > 0\n",
    "                    n_sends = n_sends + 1;\n",
    "                    # println(n_sends) \n",
    "                    # println(wt_scale_rel)\n",
    "                    # println(typeof( wt_scale_rel[n_sends] ))\n",
    "                    # println(typeof(scaled_acts_array[send]))\n",
    "\n",
    "                    long_input[(1 + n_inps):(n_inps + net.layers[send].N)] = wt_scale_rel[n_sends] .* scaled_acts_array[send]\n",
    "                    \n",
    "                    n_inps = n_inps + net.layers[send].N;\n",
    "                end\n",
    "            end\n",
    "            # now we call 'cycle'\n",
    "            # println(n_inps)\n",
    "            # println(typeof(long_input[1:n_inps]))\n",
    "            # println(typeof(inputs[recv]))\n",
    "            cycle(net.layers[recv], long_input[1:n_inps], inputs[recv]);                   \n",
    "        end\n",
    "    end\n",
    "    \n",
    "end # end of 'cycle' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1.1) Set the dimensions of the layers\n",
    "dim_lays = [(5,5), (10, 10), (5, 5)]\n",
    "# # 1.2) Specify connectivity between layers\n",
    "connections = [0  0  0;\n",
    "               1  0 .2;\n",
    "               0  1  0];\n",
    "#    # connections(i,j) = c means that layer i receives connections from j,\n",
    "#    # and that they have a relative strength c. In this case, feedback\n",
    "#    # connections are 5 times weaker. The relative weight scale of layer i\n",
    "#    # comes from the non-zero entries of row i.   \n",
    "#    # The network constructor will normalize this matrix so that if there\n",
    "#    # are non-zero entries in a row, they add to 1.\n",
    "n_lays = length(dim_lays);\n",
    "# println(n_lays)\n",
    "n_units = zeros(Int64,1,n_lays); ## number of units in each layer\n",
    "\n",
    "for i in 1:n_lays\n",
    "    n_units[i] = dim_lays[i][1] * dim_lays[i][2];\n",
    "end\n",
    "# println(n_units)\n",
    "# w0 =  zeros((n_lays,n_lays)) ## this cell will contain all the initial connection matrices\n",
    "w0 = Array{Any}(undef, (n_lays,n_lays))\n",
    "# println(size(w0))\n",
    "for rcv in 1:n_lays\n",
    "    for snd in 1:n_lays\n",
    "        # println(n_units[rcv],' ',n_units[snd])\n",
    "        if connections[rcv,snd] > 0\n",
    "            # rand(Uniform(0.4,0.6),5,5)\n",
    "            # println(rcv, \" \", snd)\n",
    "            w0[rcv,snd] = 0.3 .+ 0.4*rand(Uniform(),n_units[rcv],n_units[snd]);\n",
    "            # println(size(0.3 .+ 0.4*rand(Uniform(),n_units[rcv],n_units[snd])))\n",
    "            # println(0.3 .+ 0.4*rand(Uniform(),n_units[rcv],n_units[snd]);)\n",
    "            # # random initial weights between 0.3 and 0.7, close to 0.5\n",
    "            # # because of weight contrast enhancement            \n",
    "            \n",
    "            # w0[rcv,snd] = 0.3 + 0.4*rand()  #TODO fix this\n",
    "            # # notice that the dimensions of the layer don't matter, only\n",
    "            # # the number of units. Layers are 2-dimensional only for\n",
    "            # # purposes of visualization.    \n",
    "            # 25 25   100 25   25 25\n",
    "            # 25 100  100 100  25 100\n",
    "            # 25 25   100 25   25 25 \n",
    "        else\n",
    "            w0[rcv,snd] = 0.0\n",
    "        end\n",
    "        # println(rcv,\",\",snd,\": \",size(w0[rcv,snd]))\n",
    "    end\n",
    "end\n",
    "\n",
    "net = network(dim_lays, connections, w0);\n",
    "\n",
    "# ## 4) Let's create some inputs\n",
    "# n_inputs = 15;  # number of input-output patterns to associate\n",
    "# # patterns = cell(n_inputs,2); # patterns{i,1} is the i-th input pattern, and \n",
    "#                              # patterns{i,2} is the i-th output pattern.\n",
    "# patterns = Array{Matrix{Float64}, 2}(undef, (n_inputs,2))\n",
    "# # This will assume that layers 1 and 3 are input and output respectively.\n",
    "# # Patterns will be binary.\n",
    "# prop = 0.3; # the proportion of active units in the patterns\n",
    "# for i in 1:n_inputs\n",
    "#     # rand(Uniform(),n_units[rcv],n_units[snd]);\n",
    "#     patterns[i,1] = round.(2 .* prop .* rand(Uniform(), 1, n_units[1]));  # either 0 or 1\n",
    "#     patterns[i,2] = round.(2 .* prop .* rand(Uniform(), 1, n_units[3]));\n",
    "#     patterns[i,1] = 0.01 .+ 0.95 .* patterns[i,1];  # either 0.01 or 0.96\n",
    "#     patterns[i,2] = 0.01 .+ 0.95 .* patterns[i,2];\n",
    "# end\n",
    "\n",
    "\n",
    "n_inputs = 5;  # number of input-output patterns to associate\n",
    "patterns = Array{Matrix{Float64}, 2}(undef, (n_inputs,2)) # patterns{i,1} is the i-th input pattern, and \n",
    "                             # patterns{i,2} is the i-th output pattern.\n",
    "# This will assume that layers 1 and 3 are input and output respectively.\n",
    "# Pattern values are binary, either 0.01 or 0.99\n",
    "patterns[1,1] = repeat([0 0 1 0 0],5,1);   # vertical line\n",
    "patterns[2,1] = [1 1 1 1 1;zeros(4,5)];   # horizontal line\n",
    "patterns[3,1] = [0 0 0 0 1;0 0 0 1 0;0 0 1 0 0;0 1 0 0 0; 1 0 0 0 0]; # diagonal 1\n",
    "\n",
    "# pretty_print(patterns[3,1])\n",
    "# pretty_print(reverse(patterns[3,1], dims=2))\n",
    "# break\n",
    "patterns[4,1] = reverse(patterns[3,1], dims=2) #patterns[3,1][[5 4 3 2 1],:]   # diagonal 2\n",
    "# println(patterns[3,1])\n",
    "# println(patterns[4,1])\n",
    "# break\n",
    "patterns[5,1] =  1.0 .* (!=(0.0).(patterns[3,1]) .|| !=(0.0).(patterns[4,1]))   # two diagonals\n",
    "for i in 1:n_inputs # outputs are the same as inputs (an autoassociator)\n",
    "    patterns[i,1] = 0.01 .+ 0.98 .* patterns[i,1];\n",
    "    patterns[i,2] = patterns[i,1];\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 5) Train the network\n",
    "\n",
    "# Specify parameters for training\n",
    "n_epochs = 10;  # number of epochs. All input patterns are presented in one.\n",
    "n_trials = n_inputs; # number of trials. One input pattern per trial.\n",
    "n_minus = 50;  # number of minus cycles per trial.\n",
    "n_plus = 25; # number of plus cycles per trial.\n",
    "# lrate_sched = linspace(0.8,0.2,n_epochs); # learning rate schedule\n",
    "#LinRange(1, 100, 100) == linspace(1,100)\n",
    "lrate_sched = collect(LinRange(0.8, 0.2, n_epochs))\n",
    "\n",
    "errors = zeros(n_epochs,n_trials); # cosine error for each pattern\n",
    "\n",
    "for epoch in 1:n_epochs\n",
    "    order = randperm(n_trials); # order of presentation of inputs this epoch\n",
    "    net.lrate = lrate_sched[epoch]; # learning rate for this epoch\n",
    "# outs =  order[1]\n",
    "# pat = order[1]\n",
    "    for trial in 1:n_trials\n",
    "        reset(net);  # randomize the acts for all units\n",
    "        # println(net.layers[1].units[1])\n",
    "        # return\n",
    "        pat = order[trial];  # input to be presented this trial\n",
    "\n",
    "        #++++++ MINUS PHASE +++++++\n",
    "        inputs::Vector{Array{Float64}} = [patterns[pat, 1], [], []];\n",
    "        for minus in 1:n_minus # minus cycles: layer 1 is clamped\n",
    "            cycle(net, inputs, true);\n",
    "            # println(net.layers[1])\n",
    "            # return\n",
    "        end\n",
    "        outs = (activities(net.layers[3])) # saving the output for testing\n",
    "        # println(outs)\n",
    "        # return\n",
    "        #+++++++ PLUS PHASE +++++++\n",
    "        inputs = [patterns[pat, 1], [], patterns[pat, 2]];\n",
    "        # println(inputs)\n",
    "        # return\n",
    "        # println(inputs)\n",
    "        for plus in 1:n_plus # plus cycles: layers 1 and 3 are clamped\n",
    "            cycle(net, inputs, true);\n",
    "        end\n",
    "        \n",
    "        updt_long_avgs(net) # update averages used for net input scaling                            \n",
    "        # return\n",
    "        #+++++++ LEARNING +++++++\n",
    "        XCAL_learn(net)  # updates the avg_l vars and applies XCAL learning\n",
    "        \n",
    "        if mod(trial, 5) == 0 # display a message every X trials\n",
    "            # println(\"trial $trial finished\")\n",
    "        end        \n",
    "        \n",
    "        #+++++++ ERRORS +++++++\n",
    "        # Only the cosine error is used here\n",
    "        # println(patterns[pat, 2][:])\n",
    "        # return\n",
    "        # errors[epoch, pat] = 1 - sum(outs .* patterns[pat, 2]) / ( norm(outs) * norm(patterns[pat, 2]) );\n",
    "                                    # (25,)  (1, 25)\n",
    "        # print(size(outs))\n",
    "        # println(size(patterns[pat, 2][:]))\n",
    "        # return\n",
    "        # errors[epoch, pat] = 1 - sum(outs .* patterns[pat, 2]) / ( norm(outs) * norm(patterns[pat, 2]) );\n",
    "        errors[epoch, pat] = 1 - sum(outs .* transpose(patterns[pat, 2][:])) / ( norm(outs) * norm(transpose(patterns[pat, 2][:])) );\n",
    "        # println((outs .* patterns[pat, 2]))\n",
    "        # return\n",
    "                                  \n",
    "    end\n",
    "    # # println(outs)\n",
    "    # # println(patterns[pat, 2])\n",
    "    # # println(size(outs), size(patterns[pat, 2]))\n",
    "    # println((outs .* patterns[pat, 2]))\n",
    "    # println(sum(outs .* patterns[pat, 2]))\n",
    "    # # println(norm(outs))\n",
    "    # # println(norm(patterns[pat, 2]))\n",
    "    # # println(( norm(outs) * norm(patterns[pat, 2]) ))\n",
    "    # # println(sum(outs .* patterns[pat, 2]) / ( norm(outs) * norm(patterns[pat, 2]) ))\n",
    "    # # println(errors[epoch, pat])\n",
    "    # return\n",
    "    # println(\"epoch $epoch finished\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip220\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip220)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip221\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip220)\" d=\"\nM156.598 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.598 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip222\">\n    <rect x=\"156\" y=\"47\" width=\"2197\" height=\"1440\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip222)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  448.959,1486.45 448.959,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip222)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  909.369,1486.45 909.369,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip222)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1369.78,1486.45 1369.78,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip222)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1830.19,1486.45 1830.19,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip222)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2290.6,1486.45 2290.6,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  448.959,1486.45 448.959,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  909.369,1486.45 909.369,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1369.78,1486.45 1369.78,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1830.19,1486.45 1830.19,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2290.6,1486.45 2290.6,1467.55 \n  \"/>\n<path clip-path=\"url(#clip220)\" d=\"M443.612 1544.91 L459.931 1544.91 L459.931 1548.85 L437.987 1548.85 L437.987 1544.91 Q440.649 1542.16 445.232 1537.53 Q449.838 1532.88 451.019 1531.53 Q453.264 1529.01 454.144 1527.27 Q455.047 1525.51 455.047 1523.82 Q455.047 1521.07 453.102 1519.33 Q451.181 1517.6 448.079 1517.6 Q445.88 1517.6 443.426 1518.36 Q440.996 1519.13 438.218 1520.68 L438.218 1515.95 Q441.042 1514.82 443.496 1514.24 Q445.95 1513.66 447.987 1513.66 Q453.357 1513.66 456.551 1516.35 Q459.746 1519.03 459.746 1523.52 Q459.746 1525.65 458.936 1527.57 Q458.149 1529.47 456.042 1532.07 Q455.463 1532.74 452.362 1535.95 Q449.26 1539.15 443.612 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M912.379 1518.36 L900.573 1536.81 L912.379 1536.81 L912.379 1518.36 M911.152 1514.29 L917.031 1514.29 L917.031 1536.81 L921.962 1536.81 L921.962 1540.7 L917.031 1540.7 L917.031 1548.85 L912.379 1548.85 L912.379 1540.7 L896.777 1540.7 L896.777 1536.19 L911.152 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M1370.18 1529.7 Q1367.04 1529.7 1365.18 1531.86 Q1363.36 1534.01 1363.36 1537.76 Q1363.36 1541.49 1365.18 1543.66 Q1367.04 1545.82 1370.18 1545.82 Q1373.33 1545.82 1375.16 1543.66 Q1377.01 1541.49 1377.01 1537.76 Q1377.01 1534.01 1375.16 1531.86 Q1373.33 1529.7 1370.18 1529.7 M1379.47 1515.05 L1379.47 1519.31 Q1377.71 1518.48 1375.9 1518.04 Q1374.12 1517.6 1372.36 1517.6 Q1367.73 1517.6 1365.28 1520.72 Q1362.85 1523.85 1362.5 1530.17 Q1363.87 1528.15 1365.93 1527.09 Q1367.99 1526 1370.46 1526 Q1375.67 1526 1378.68 1529.17 Q1381.71 1532.32 1381.71 1537.76 Q1381.71 1543.08 1378.56 1546.3 Q1375.42 1549.52 1370.18 1549.52 Q1364.19 1549.52 1361.02 1544.94 Q1357.85 1540.33 1357.85 1531.6 Q1357.85 1523.41 1361.74 1518.55 Q1365.62 1513.66 1372.18 1513.66 Q1373.93 1513.66 1375.72 1514.01 Q1377.52 1514.36 1379.47 1515.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M1830.19 1532.44 Q1826.86 1532.44 1824.94 1534.22 Q1823.04 1536 1823.04 1539.13 Q1823.04 1542.25 1824.94 1544.03 Q1826.86 1545.82 1830.19 1545.82 Q1833.52 1545.82 1835.44 1544.03 Q1837.37 1542.23 1837.37 1539.13 Q1837.37 1536 1835.44 1534.22 Q1833.55 1532.44 1830.19 1532.44 M1825.51 1530.45 Q1822.5 1529.7 1820.82 1527.64 Q1819.15 1525.58 1819.15 1522.62 Q1819.15 1518.48 1822.09 1516.07 Q1825.05 1513.66 1830.19 1513.66 Q1835.35 1513.66 1838.29 1516.07 Q1841.23 1518.48 1841.23 1522.62 Q1841.23 1525.58 1839.54 1527.64 Q1837.88 1529.7 1834.89 1530.45 Q1838.27 1531.23 1840.14 1533.52 Q1842.04 1535.82 1842.04 1539.13 Q1842.04 1544.15 1838.96 1546.83 Q1835.91 1549.52 1830.19 1549.52 Q1824.47 1549.52 1821.39 1546.83 Q1818.34 1544.15 1818.34 1539.13 Q1818.34 1535.82 1820.24 1533.52 Q1822.13 1531.23 1825.51 1530.45 M1823.8 1523.06 Q1823.8 1525.75 1825.47 1527.25 Q1827.16 1528.76 1830.19 1528.76 Q1833.2 1528.76 1834.89 1527.25 Q1836.6 1525.75 1836.6 1523.06 Q1836.6 1520.38 1834.89 1518.87 Q1833.2 1517.37 1830.19 1517.37 Q1827.16 1517.37 1825.47 1518.87 Q1823.8 1520.38 1823.8 1523.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M2265.29 1544.91 L2272.93 1544.91 L2272.93 1518.55 L2264.62 1520.21 L2264.62 1515.95 L2272.88 1514.29 L2277.56 1514.29 L2277.56 1544.91 L2285.2 1544.91 L2285.2 1548.85 L2265.29 1548.85 L2265.29 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M2304.64 1517.37 Q2301.03 1517.37 2299.2 1520.93 Q2297.39 1524.47 2297.39 1531.6 Q2297.39 1538.71 2299.2 1542.27 Q2301.03 1545.82 2304.64 1545.82 Q2308.27 1545.82 2310.08 1542.27 Q2311.91 1538.71 2311.91 1531.6 Q2311.91 1524.47 2310.08 1520.93 Q2308.27 1517.37 2304.64 1517.37 M2304.64 1513.66 Q2310.45 1513.66 2313.51 1518.27 Q2316.58 1522.85 2316.58 1531.6 Q2316.58 1540.33 2313.51 1544.94 Q2310.45 1549.52 2304.64 1549.52 Q2298.83 1549.52 2295.75 1544.94 Q2292.7 1540.33 2292.7 1531.6 Q2292.7 1522.85 2295.75 1518.27 Q2298.83 1513.66 2304.64 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip222)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  156.598,1454.05 2352.76,1454.05 \n  \"/>\n<polyline clip-path=\"url(#clip222)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  156.598,1094.53 2352.76,1094.53 \n  \"/>\n<polyline clip-path=\"url(#clip222)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  156.598,735.022 2352.76,735.022 \n  \"/>\n<polyline clip-path=\"url(#clip222)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  156.598,375.511 2352.76,375.511 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,1486.45 156.598,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,1454.05 175.496,1454.05 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,1094.53 175.496,1094.53 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,735.022 175.496,735.022 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  156.598,375.511 175.496,375.511 \n  \"/>\n<path clip-path=\"url(#clip220)\" d=\"M63.4226 1439.84 Q59.8115 1439.84 57.9828 1443.41 Q56.1773 1446.95 56.1773 1454.08 Q56.1773 1461.19 57.9828 1464.75 Q59.8115 1468.29 63.4226 1468.29 Q67.0569 1468.29 68.8624 1464.75 Q70.6911 1461.19 70.6911 1454.08 Q70.6911 1446.95 68.8624 1443.41 Q67.0569 1439.84 63.4226 1439.84 M63.4226 1436.14 Q69.2328 1436.14 72.2883 1440.75 Q75.367 1445.33 75.367 1454.08 Q75.367 1462.81 72.2883 1467.41 Q69.2328 1472 63.4226 1472 Q57.6125 1472 54.5338 1467.41 Q51.4782 1462.81 51.4782 1454.08 Q51.4782 1445.33 54.5338 1440.75 Q57.6125 1436.14 63.4226 1436.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M83.5845 1465.45 L88.4688 1465.45 L88.4688 1471.33 L83.5845 1471.33 L83.5845 1465.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M108.654 1439.84 Q105.043 1439.84 103.214 1443.41 Q101.409 1446.95 101.409 1454.08 Q101.409 1461.19 103.214 1464.75 Q105.043 1468.29 108.654 1468.29 Q112.288 1468.29 114.094 1464.75 Q115.922 1461.19 115.922 1454.08 Q115.922 1446.95 114.094 1443.41 Q112.288 1439.84 108.654 1439.84 M108.654 1436.14 Q114.464 1436.14 117.52 1440.75 Q120.598 1445.33 120.598 1454.08 Q120.598 1462.81 117.52 1467.41 Q114.464 1472 108.654 1472 Q102.844 1472 99.765 1467.41 Q96.7095 1462.81 96.7095 1454.08 Q96.7095 1445.33 99.765 1440.75 Q102.844 1436.14 108.654 1436.14 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M65.0198 1080.33 Q61.4087 1080.33 59.58 1083.9 Q57.7745 1087.44 57.7745 1094.57 Q57.7745 1101.68 59.58 1105.24 Q61.4087 1108.78 65.0198 1108.78 Q68.6541 1108.78 70.4596 1105.24 Q72.2883 1101.68 72.2883 1094.57 Q72.2883 1087.44 70.4596 1083.9 Q68.6541 1080.33 65.0198 1080.33 M65.0198 1076.63 Q70.83 1076.63 73.8855 1081.24 Q76.9642 1085.82 76.9642 1094.57 Q76.9642 1103.3 73.8855 1107.9 Q70.83 1112.49 65.0198 1112.49 Q59.2097 1112.49 56.131 1107.9 Q53.0754 1103.3 53.0754 1094.57 Q53.0754 1085.82 56.131 1081.24 Q59.2097 1076.63 65.0198 1076.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M85.1818 1105.93 L90.066 1105.93 L90.066 1111.81 L85.1818 1111.81 L85.1818 1105.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M104.279 1107.88 L120.598 1107.88 L120.598 1111.81 L98.6539 1111.81 L98.6539 1107.88 Q101.316 1105.12 105.899 1100.49 Q110.506 1095.84 111.686 1094.5 Q113.932 1091.98 114.811 1090.24 Q115.714 1088.48 115.714 1086.79 Q115.714 1084.04 113.77 1082.3 Q111.848 1080.56 108.746 1080.56 Q106.547 1080.56 104.094 1081.33 Q101.663 1082.09 98.8854 1083.64 L98.8854 1078.92 Q101.709 1077.79 104.163 1077.21 Q106.617 1076.63 108.654 1076.63 Q114.024 1076.63 117.219 1079.31 Q120.413 1082 120.413 1086.49 Q120.413 1088.62 119.603 1090.54 Q118.816 1092.44 116.709 1095.03 Q116.131 1095.7 113.029 1098.92 Q109.927 1102.12 104.279 1107.88 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M62.9365 720.821 Q59.3254 720.821 57.4967 724.386 Q55.6912 727.928 55.6912 735.057 Q55.6912 742.164 57.4967 745.728 Q59.3254 749.27 62.9365 749.27 Q66.5707 749.27 68.3763 745.728 Q70.205 742.164 70.205 735.057 Q70.205 727.928 68.3763 724.386 Q66.5707 720.821 62.9365 720.821 M62.9365 717.117 Q68.7467 717.117 71.8022 721.724 Q74.8809 726.307 74.8809 735.057 Q74.8809 743.784 71.8022 748.39 Q68.7467 752.974 62.9365 752.974 Q57.1264 752.974 54.0477 748.39 Q50.9921 743.784 50.9921 735.057 Q50.9921 726.307 54.0477 721.724 Q57.1264 717.117 62.9365 717.117 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M83.0984 746.423 L87.9827 746.423 L87.9827 752.302 L83.0984 752.302 L83.0984 746.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M111.015 721.817 L99.2095 740.266 L111.015 740.266 L111.015 721.817 M109.788 717.742 L115.668 717.742 L115.668 740.266 L120.598 740.266 L120.598 744.154 L115.668 744.154 L115.668 752.302 L111.015 752.302 L111.015 744.154 L95.4132 744.154 L95.4132 739.641 L109.788 717.742 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M63.2606 361.31 Q59.6495 361.31 57.8208 364.874 Q56.0152 368.416 56.0152 375.546 Q56.0152 382.652 57.8208 386.217 Q59.6495 389.759 63.2606 389.759 Q66.8948 389.759 68.7004 386.217 Q70.5291 382.652 70.5291 375.546 Q70.5291 368.416 68.7004 364.874 Q66.8948 361.31 63.2606 361.31 M63.2606 357.606 Q69.0707 357.606 72.1263 362.212 Q75.205 366.796 75.205 375.546 Q75.205 384.272 72.1263 388.879 Q69.0707 393.462 63.2606 393.462 Q57.4504 393.462 54.3717 388.879 Q51.3162 384.272 51.3162 375.546 Q51.3162 366.796 54.3717 362.212 Q57.4504 357.606 63.2606 357.606 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M83.4225 386.911 L88.3067 386.911 L88.3067 392.791 L83.4225 392.791 L83.4225 386.911 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M109.071 373.648 Q105.922 373.648 104.071 375.8 Q102.242 377.953 102.242 381.703 Q102.242 385.43 104.071 387.606 Q105.922 389.759 109.071 389.759 Q112.219 389.759 114.047 387.606 Q115.899 385.43 115.899 381.703 Q115.899 377.953 114.047 375.8 Q112.219 373.648 109.071 373.648 M118.353 358.995 L118.353 363.254 Q116.594 362.421 114.788 361.981 Q113.006 361.541 111.246 361.541 Q106.617 361.541 104.163 364.666 Q101.733 367.791 101.385 374.11 Q102.751 372.097 104.811 371.032 Q106.871 369.944 109.348 369.944 Q114.557 369.944 117.566 373.115 Q120.598 376.263 120.598 381.703 Q120.598 387.027 117.45 390.245 Q114.302 393.462 109.071 393.462 Q103.075 393.462 99.9039 388.879 Q96.7326 384.272 96.7326 375.546 Q96.7326 367.351 100.621 362.49 Q104.51 357.606 111.061 357.606 Q112.82 357.606 114.603 357.953 Q116.408 358.3 118.353 358.995 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip222)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  218.754,87.9763 448.959,829.699 679.164,1201.56 909.369,1381.07 1139.57,1400.96 1369.78,1426.14 1599.98,1431.9 1830.19,1432.56 2060.4,1437.53 2290.6,1445.72 \n  \n  \"/>\n<path clip-path=\"url(#clip220)\" d=\"\nM1983.1 198.898 L2279.55 198.898 L2279.55 95.2176 L1983.1 95.2176  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1983.1,198.898 2279.55,198.898 2279.55,95.2176 1983.1,95.2176 1983.1,198.898 \n  \"/>\n<polyline clip-path=\"url(#clip220)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2007.5,147.058 2153.92,147.058 \n  \"/>\n<path clip-path=\"url(#clip220)\" d=\"M2192.16 166.745 Q2190.35 171.375 2188.64 172.787 Q2186.93 174.199 2184.06 174.199 L2180.65 174.199 L2180.65 170.634 L2183.15 170.634 Q2184.91 170.634 2185.89 169.8 Q2186.86 168.967 2188.04 165.865 L2188.8 163.921 L2178.32 138.412 L2182.83 138.412 L2190.93 158.689 L2199.03 138.412 L2203.55 138.412 L2192.16 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip220)\" d=\"M2210.84 160.402 L2218.48 160.402 L2218.48 134.037 L2210.17 135.703 L2210.17 131.444 L2218.43 129.778 L2223.11 129.778 L2223.11 160.402 L2230.75 160.402 L2230.75 164.338 L2210.84 164.338 L2210.84 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_errs = mean(errors, dims=2);\n",
    "plot(mean_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching |(::Float64, ::Float64)\n\u001b[0mClosest candidates are:\n\u001b[0m  |(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at ~/.julia/juliaup/julia-1.8.0-beta1+0~x64/share/julia/base/operators.jl:591\n\u001b[0m  |(\u001b[91m::DataValues.DataValue{T1}\u001b[39m, ::T2) where {T1<:Number, T2<:Number} at ~/.julia/packages/DataValues/N7oeL/src/scalar/core.jl:212\n\u001b[0m  |(\u001b[91m::DataValues.DataValue{T1}\u001b[39m, ::T2) where {T1, T2} at ~/.julia/packages/DataValues/N7oeL/src/scalar/operations.jl:65\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching |(::Float64, ::Float64)\n\u001b[0mClosest candidates are:\n\u001b[0m  |(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at ~/.julia/juliaup/julia-1.8.0-beta1+0~x64/share/julia/base/operators.jl:591\n\u001b[0m  |(\u001b[91m::DataValues.DataValue{T1}\u001b[39m, ::T2) where {T1<:Number, T2<:Number} at ~/.julia/packages/DataValues/N7oeL/src/scalar/core.jl:212\n\u001b[0m  |(\u001b[91m::DataValues.DataValue{T1}\u001b[39m, ::T2) where {T1, T2} at ~/.julia/packages/DataValues/N7oeL/src/scalar/operations.jl:65\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      " [1] _broadcast_getindex_evalf",
      "   @ ./broadcast.jl:670 [inlined]",
      " [2] _broadcast_getindex",
      "   @ ./broadcast.jl:643 [inlined]",
      " [3] getindex",
      "   @ ./broadcast.jl:597 [inlined]",
      " [4] copy",
      "   @ ./broadcast.jl:899 [inlined]",
      " [5] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2}, Nothing, typeof(|), Tuple{Matrix{Float64}, Matrix{Float64}}})",
      "   @ Base.Broadcast ./broadcast.jl:860",
      " [6] top-level scope",
      "   @ In[150]:1",
      " [7] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1277"
     ]
    }
   ],
   "source": [
    "[0.0 0.0 0.0 0.0 1.0; 0.0 0.0 0.0 1.0 0.0; 0.0 0.0 1.0 0.0 0.0; 0.0 1.0 0.0 0.0 0.0; 1.0 0.0 0.0 0.0 0.0] .| [1.0 0.0 0.0 0.0 0.0; 0.0 1.0 0.0 0.0 0.0; 0.0 0.0 1.0 0.0 0.0; 0.0 0.0 0.0 1.0 0.0; 0.0 0.0 0.0 0.0 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-beta1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
